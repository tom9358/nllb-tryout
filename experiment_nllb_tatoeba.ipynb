{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration saved to: models/nllb-200-distilled-1.3B-nld-gos-20250331-193121/config.txt\n",
      "Model save path: models/nllb-200-distilled-1.3B-nld-gos-20250331-193121\n",
      "Downloading necessary Tatoeba files...\n",
      "Links CSV already exists. Skipping download and unpacking.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 13127.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tatoeba/nld_sentences.tsv already exists. Skipping download and unpacking.\n",
      "tatoeba/gos_sentences.tsv already exists. Skipping download and unpacking.\n",
      "Setting up parallel corpus for nld_Latn gos_Latn\n",
      "Downloading necessary Tatoeba files...\n",
      "Links CSV already exists. Skipping download and unpacking.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 21399.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tatoeba/nld_sentences.tsv already exists. Skipping download and unpacking.\n",
      "tatoeba/gos_sentences.tsv already exists. Skipping download and unpacking.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from config import config\n",
    "from downloadtatoeba import main_download\n",
    "from corpus import main_corpus\n",
    "from evaluate import main_evaluate\n",
    "\n",
    "\n",
    "# Step 1: Download data\n",
    "main_download(config[\"source_langs_tatoeba\"])\n",
    "\n",
    "# Step 2: Load and create parallel corpus\n",
    "corpus_objects = main_corpus(config[\"source_langs_tatoeba\"], config[\"source_langs_nllb\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.is_available(): True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tom.brand/Offline/nllb-tryout/nllb-tat/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/tom.brand/Offline/nllb-tryout/nllb-tat/lib/python3.10/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/tom.brand/Offline/nllb-tryout/nllb-tat/lib/python3.10/site-packages/transformers/modeling_utils.py:488: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=map_location)\n",
      "/home/tom.brand/Offline/nllb-tryout/nllb-tat/lib/python3.10/site-packages/transformers/modeling_utils.py:488: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=map_location)\n",
      "/home/tom.brand/Offline/nllb-tryout/nllb-tat/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 256205. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized weights for gos_Latn equal to those of nld_Latn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps: 100%|██████████| 800/800 [07:44<00:00,  1.72it/s, loss=0.415]\n"
     ]
    }
   ],
   "source": [
    "from train import main_train\n",
    "# Step 3: Train the model\n",
    "main_train(corpus_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from models/facebook/nllb-200-distilled-1.3B-nld-gos-20250331-182640/799...\n",
      "torch.cuda.is_available(): True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tom.brand/Offline/nllb-tryout/nllb-tat/lib/python3.10/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/tom.brand/Offline/nllb-tryout/nllb-tat/lib/python3.10/site-packages/transformers/modeling_utils.py:488: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=map_location)\n",
      "/home/tom.brand/Offline/nllb-tryout/nllb-tat/lib/python3.10/site-packages/transformers/modeling_utils.py:488: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=map_location)\n",
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 256205. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n",
      "Enter text to translate. Type 'END' to exit or use 'LANG src tgt' to change languages. Format e.g. nld_Latn gos_Latn\n",
      "Translate (nld_Latn -> gos_Latn): ik ben benieuwd of dit werkt.\n",
      "Translation: ik bin nieuwsgierig of dit waarkt.\n",
      "Translate (nld_Latn -> gos_Latn): jij bent het zusje van mijn buurman!\n",
      "Translation: doe bist t zuske van mien noaber!\n",
      "Translate (nld_Latn -> gos_Latn): onverwachts kwamen de ouders van Bert op bezoek.\n",
      "Translation: onwaarschijnlijk kwammen olders van Bert op veziede.\n",
      "Translate (nld_Latn -> gos_Latn): de straatkat is een kat die op straat leeft.\n",
      "Translation: stroatkat is n kadde dij op stroat leeft.\n",
      "Translate (nld_Latn -> gos_Latn): hebben jullie dat gezegd?\n",
      "Translation: hebben joe dat zègd?\n",
      "Translate (nld_Latn -> gos_Latn): de molenaar ziet jullie.\n",
      "Translation: mulder zain joe.\n",
      "Translate (nld_Latn -> gos_Latn): ik weet niet wat je bedoelt...\n",
      "Translation: k Wait nait watst bedoulst...\n",
      "Translate (nld_Latn -> gos_Latn): wiens linkerarm is het sterkste? die van mij.\n",
      "Translation: wel zien linker aarm is t staarkst? Dij van mie.\n",
      "Exiting translation tool.\n"
     ]
    }
   ],
   "source": [
    "from tryout import main_tryout\n",
    "MODEL_SAVE_PATH = \"models/facebook/nllb-200-distilled-1.3B-nld-gos-20250331-182640\" # config[\"MODEL_SAVE_PATH\"]\n",
    "\n",
    "inputs = [\n",
    "    \"ik ben benieuwd of dit werkt.\",\n",
    "    \"jij bent het zusje van mijn buurman!\",\n",
    "    \"onverwachts kwamen de ouders van Bert op bezoek.\",\n",
    "    \"de straatkat is een kat die op straat leeft.\",\n",
    "    \"hebben jullie dat gezegd?\",\n",
    "    \"de molenaar ziet jullie.\",\n",
    "    \"ik weet niet wat je bedoelt...\",\n",
    "    \"wiens linkerarm is het sterkste? die van mij.\"\n",
    "]\n",
    "\n",
    "main_tryout(MODEL_SAVE_PATH, config[\"new_lang_nllb\"], inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['799']\n",
      "Loading model from models/facebook/nllb-200-distilled-1.3B-nld-gos-20250331-182640/799...\n",
      "torch.cuda.is_available(): True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tom.brand/Offline/nllb-tryout/nllb-tat/lib/python3.10/site-packages/transformers/modeling_utils.py:488: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=map_location)\n",
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 256205. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from tokenizer_and_model_setup import setup_model_and_tokenizer\n",
    "\n",
    "model_versions = [\n",
    "    d for d in os.listdir(MODEL_SAVE_PATH)\n",
    "    if os.path.isdir(os.path.join(MODEL_SAVE_PATH, d))\n",
    "]\n",
    "model_versions.sort(key=lambda x: int(x))\n",
    "print(model_versions)\n",
    "latest_model = model_versions[-1]\n",
    "model_path = os.path.join(MODEL_SAVE_PATH, latest_model)\n",
    "print(f\"Loading model from {model_path}...\")\n",
    "model, tokenizer = setup_model_and_tokenizer(model_path, new_lang=config[\"new_lang_nllb\"])\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_hub.login(token=\"hf_something\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_repo = \"Tom9358/nllb-tatoeba-gos-nld-v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece.bpe.model: 100%|██████████| 4.85M/4.85M [00:01<00:00, 4.07MB/s]\n",
      "pytorch_model.bin: 100%|██████████| 5.48G/5.48G [03:00<00:00, 30.4MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Tom9358/nllb-tatoeba-gos-nld-v1/commit/22fd6747b4ebc60d0199131fe3f5677fc8211c34', commit_message='Upload M2M100ForConditionalGeneration', commit_description='', oid='22fd6747b4ebc60d0199131fe3f5677fc8211c34', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Tom9358/nllb-tatoeba-gos-nld-v1', endpoint='https://huggingface.co', repo_type='model', repo_id='Tom9358/nllb-tatoeba-gos-nld-v1'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(upload_repo)\n",
    "model.push_to_hub(upload_repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eerste niveau in 'models' bevat:\n",
      "['nllb-200-distilled-1.3B-nld-gos-20250315-203450', 'nllb-200-distilled-1.3B-nld-gos-20250315-203832', 'nllb-200-distilled-1.3B-nld-gos-20250329-113115', 'nllb-200-distilled-1.3B-nld-gos-20250315-203303', 'nllb-200-distilled-1.3B-nld-gos-20250315-204002']\n",
      "\n",
      "Tweede niveau in 'nllb-200-distilled-1.3B-nld-gos-20250315-203450' bevat:\n",
      "['config.txt']\n",
      "\n",
      "Tweede niveau in 'nllb-200-distilled-1.3B-nld-gos-20250315-203832' bevat:\n",
      "['config.txt']\n",
      "\n",
      "Tweede niveau in 'nllb-200-distilled-1.3B-nld-gos-20250329-113115' bevat:\n",
      "['config.txt']\n",
      "\n",
      "Tweede niveau in 'nllb-200-distilled-1.3B-nld-gos-20250315-203303' bevat:\n",
      "['config.txt']\n",
      "\n",
      "Tweede niveau in 'nllb-200-distilled-1.3B-nld-gos-20250315-204002' bevat:\n",
      "['config.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def print_directory_structure(base_dir: str):\n",
    "    \"\"\"\n",
    "    Print de structuur van mappen binnen de basisdirectory, tot twee niveaus diep.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        first_level = os.listdir(base_dir)\n",
    "        print(f\"Eerste niveau in '{base_dir}' bevat:\")\n",
    "        print(first_level)\n",
    "\n",
    "        for first_level_dir in first_level:\n",
    "            first_level_path = os.path.join(base_dir, first_level_dir)\n",
    "            if os.path.isdir(first_level_path):\n",
    "                second_level = os.listdir(first_level_path)\n",
    "                print(f\"\\nTweede niveau in '{first_level_dir}' bevat:\")\n",
    "                print(second_level)\n",
    "    except Exception as e:\n",
    "        print(f'Er is een fout opgetreden: {e}')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_directory = 'models'\n",
    "    print_directory_structure(base_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nllb-tat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
