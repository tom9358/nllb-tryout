{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Initial conditions and Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "\n",
    "modelpath = '/hfacemodels'\n",
    "modelname = 'facebook/nllb-200-distilled-1.3B' # 3.3B might be too big for my puny 3060 12GB (1.3B was too big for 1660ti). Might also be worse if it's not distilled.\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "MODEL_SAVE_PATH = f'/models/nllb-nld-gos-distilled-1.3B-{timestamp}'\n",
    "print(MODEL_SAVE_PATH)\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def download_tatoeba(source_lang: str, trt_lang: str, redownload: bool = False):\n",
    "    links = [\n",
    "        rf'https://downloads.tatoeba.org/exports/per_language/{source_lang}/{source_lang}_sentences.tsv.bz2',\n",
    "        rf'https://downloads.tatoeba.org/exports/per_language/{trt_lang}/{trt_lang}_sentences.tsv.bz2',\n",
    "        r'https://downloads.tatoeba.org/exports/links.tar.bz2'\n",
    "    ]\n",
    "    files = [\n",
    "        source_lang + \"_sentences.tsv.bz2\",\n",
    "        trt_lang + \"_sentences.tsv.bz2\",\n",
    "        \"links.tar.bz2\"\n",
    "    ]\n",
    "    \n",
    "    for link, file in zip(links, files):\n",
    "        if not os.path.exists(file) or redownload:\n",
    "            print(f'Downloading {file}...')\n",
    "            data = requests.get(link)\n",
    "            open(file, \"wb\").write(data.content)\n",
    "        else:\n",
    "            print(f'File {file} already exists. Skipping download.')\n",
    "\n",
    "# Define language codes\n",
    "source_langs = [\"nld\", \"gos\", \"eng\", \"deu\"]\n",
    "target_langs = source_langs\n",
    "\n",
    "# Download the necessary files once\n",
    "for src_lang in tqdm(source_langs):\n",
    "    download_tatoeba(src_lang, src_lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OqdSSIVLlCir"
   },
   "source": [
    "## 1. Exploring the data\n",
    "\n",
    "In this section, I try to understand what is the training data that I have, and how suitable it is for fine-tuning a NLLB model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "class ParallelCorpus:\n",
    "    def __init__(self, source_lang, target_lang):\n",
    "        self.source_lang = source_lang\n",
    "        self.target_lang = target_lang\n",
    "        self.source_lang_long = source_lang + '_Latn'\n",
    "        self.target_lang_long = target_lang + '_Latn'\n",
    "        \n",
    "        self.df = self.load_and_format_parallel_sentences()\n",
    "        \n",
    "        self.df_train, self.df_temp = train_test_split(self.df, test_size=0.02, random_state=9358)\n",
    "        self.df_validate, self.df_test = train_test_split(self.df_temp, test_size=0.5, random_state=9358)\n",
    "        # Clean up temporary DataFrame\n",
    "        del self.df_temp\n",
    "\n",
    "    def load_and_format_parallel_sentences(self):\n",
    "        df_parallel = load_tatoeba(self.source_lang, self.target_lang)\n",
    "        df_parallel[\"source_lang_code\"] = self.source_lang\n",
    "        return df_parallel\n",
    "\n",
    "def load_tatoeba(source_lang: str, trt_lang: str, max_pairs: int = None):\n",
    "    # Assume files are already downloaded\n",
    "    src_sentences = pd.read_csv(source_lang+\"_sentences.tsv.bz2\", sep=\"\\t\", header=None, names=[\"id\", \"language\", source_lang], compression='bz2')\n",
    "    trg_sentences = pd.read_csv(trt_lang+\"_sentences.tsv.bz2\", sep=\"\\t\", header=None, names=[\"id\", \"language\", trt_lang], compression='bz2')\n",
    "    link_sentences = pd.read_csv(\"links.tar.bz2\", sep=\"\\t\", header=None, names=[\"origin\", \"translation\"])\n",
    "\n",
    "    df_parallel = link_sentences \\\n",
    "        .merge(trg_sentences, left_on=\"origin\", right_on=\"id\") \\\n",
    "        .merge(src_sentences, left_on=\"translation\", right_on=\"id\")[[\"origin\", \"translation\", trt_lang, source_lang]]\n",
    "    \n",
    "    if df_parallel.isnull().any().any():\n",
    "        print('null entries found')\n",
    "    \n",
    "    print(f'For the language pair {source_lang}-{trt_lang}, Tatoeba currently contains {len(df_parallel)} direct translations.')\n",
    "\n",
    "    if max_pairs:\n",
    "        if max_pairs <= len(df_parallel):\n",
    "            df_parallel = df_parallel.sample(max_pairs)\n",
    "    \n",
    "    return df_parallel[[trt_lang, source_lang]]\n",
    "\n",
    "# Create a list of ParallelCorpus instances for each source language\n",
    "corpus_objects = []\n",
    "for i, src_lang in tqdm(enumerate(source_langs)):\n",
    "    for tgt_lang in source_langs[i+1:]:\n",
    "        print('loading parallel corpus for', src_lang, tgt_lang)\n",
    "        corpus_objects.append(ParallelCorpus(src_lang, tgt_lang))\n",
    "\n",
    "# Example: Access the training, validation, and test DataFrames for the first source language in the list\n",
    "for corpus_obj in corpus_objects:\n",
    "    display(corpus_obj.df_test.sample(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hUhun80t5u9"
   },
   "source": [
    "# 4. Adding a new language tag to the tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MhG4XWTP-g3w"
   },
   "outputs": [],
   "source": [
    "new_lang_long = \"gos_Latn\"\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, NllbTokenizer\n",
    "from gc import collect\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# we need transformers<=4.33 for this to work.\n",
    "\n",
    "def fix_tokenizer(tokenizer, new_lang='gos_Latn'):\n",
    "    \"\"\"\n",
    "    Add a new language token to the tokenizer vocabulary\n",
    "    (this should be done each time after its initialization)\n",
    "    \"\"\"\n",
    "    old_len = len(tokenizer) - int(new_lang in tokenizer.added_tokens_encoder)\n",
    "    tokenizer.lang_code_to_id[new_lang] = old_len-1\n",
    "    tokenizer.id_to_lang_code[old_len-1] = new_lang\n",
    "    # always move \"mask\" to the last position\n",
    "    tokenizer.fairseq_tokens_to_ids[\"<mask>\"] = len(tokenizer.sp_model) + len(tokenizer.lang_code_to_id) + tokenizer.fairseq_offset\n",
    "\n",
    "    tokenizer.fairseq_tokens_to_ids.update(tokenizer.lang_code_to_id)\n",
    "    tokenizer.fairseq_ids_to_tokens = {v: k for k, v in tokenizer.fairseq_tokens_to_ids.items()}\n",
    "    if new_lang not in tokenizer._additional_special_tokens:\n",
    "        tokenizer._additional_special_tokens.append(new_lang)\n",
    "    # clear the added token encoder; otherwise a new token may end up there by mistake\n",
    "    tokenizer.added_tokens_encoder = {}\n",
    "    tokenizer.added_tokens_decoder = {}\n",
    "\n",
    "def cleanup():\n",
    "    \"\"\"Try to free GPU memory\"\"\"\n",
    "    collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "tokenizer = NllbTokenizer.from_pretrained(modelname, cache_dir=modelpath)\n",
    "print(tokenizer.convert_ids_to_tokens([256202, 256203]))\n",
    "\n",
    "fix_tokenizer(tokenizer)\n",
    "print(tokenizer.convert_ids_to_tokens([256202, 256203, 256204])) # ['zul_Latn', 'tyv_Cyrl', '<mask>']\n",
    "print(tokenizer.convert_tokens_to_ids(['zul_Latn', 'gos_Latn', '<mask>'])) # [256202, 256203, 256204]\n",
    "\n",
    "cleanup()\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(modelname, cache_dir=modelpath, device_map={\"\": \"cuda:0\"})\n",
    "# for name, module in model.named_modules(): print(name)\n",
    "\n",
    "added_token_id = tokenizer.convert_tokens_to_ids(new_lang_long)\n",
    "#similar_lang_id = tokenizer.convert_tokens_to_ids('fry_Latn')\n",
    "#similar_lang_id = tokenizer.convert_tokens_to_ids('nds_Latn')\n",
    "similar_lang_id = tokenizer.convert_tokens_to_ids('nld_Latn')\n",
    "print(added_token_id, similar_lang_id)\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# moving the embedding for \"mask\" to its new position\n",
    "model.model.shared.weight.data[added_token_id+1] = model.model.shared.weight.data[added_token_id]\n",
    "# initializing new language token with a token of a similar language\n",
    "model.model.shared.weight.data[added_token_id] = model.model.shared.weight.data[similar_lang_id]\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ssJCguZ-3oH"
   },
   "source": [
    "# 5. Preparing the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OjuuYbpG-7nS"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from tqdm.auto import  trange\n",
    "from transformers.optimization import Adafactor\n",
    "from transformers import get_constant_schedule_with_warmup\n",
    "import sys\n",
    "import re\n",
    "import typing as tp\n",
    "import unicodedata\n",
    "from sacremoses import MosesPunctNormalizer\n",
    "\n",
    "model.cuda()\n",
    "\n",
    "optimizer = Adafactor(\n",
    "    [p for p in model.parameters() if p.requires_grad],\n",
    "    scale_parameter=False,\n",
    "    relative_step=False,\n",
    "    lr=1e-4,\n",
    "    clip_threshold=1.0,\n",
    "    weight_decay=1e-3,\n",
    ")\n",
    "\n",
    "batch_size = 25 # 20 is fine\n",
    "max_chars = 200 # characters. can be set to None in which case sentences will not be cut at the end. in the future the cutting might be done at the last space/word boundary before max_chars rather than in the middle of a word.\n",
    "max_length = 99 # tokens\n",
    "warmup_steps = 100\n",
    "training_steps = int(2000*8/batch_size) # my impression was that around int(1200*8/batch_size) was enough but let's set it high for overfitting investigation purposes\n",
    "\n",
    "losses = []\n",
    "scheduler = get_constant_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps)\n",
    "\n",
    "mpn = MosesPunctNormalizer(lang=\"en\")\n",
    "mpn.substitutions = [\n",
    "    (re.compile(r), sub) for r, sub in mpn.substitutions\n",
    "]\n",
    "\n",
    "def get_non_printing_char_replacer(replace_by: str = \" \") -> tp.Callable[[str], str]:\n",
    "    non_printable_map = {\n",
    "        ord(c): replace_by\n",
    "        for c in (chr(i) for i in range(sys.maxunicode + 1))\n",
    "        # same as \\p{C} in perl\n",
    "        # see https://www.unicode.org/reports/tr44/#General_Category_Values\n",
    "        if unicodedata.category(c) in {\"C\", \"Cc\", \"Cf\", \"Cs\", \"Co\", \"Cn\"}\n",
    "    }\n",
    "\n",
    "    def replace_non_printing_char(line) -> str:\n",
    "        return line.translate(non_printable_map)\n",
    "\n",
    "    return replace_non_printing_char\n",
    "\n",
    "replace_nonprint = get_non_printing_char_replacer(\" \")\n",
    "\n",
    "def preproc(text):\n",
    "    clean = mpn.normalize(text)\n",
    "    clean = replace_nonprint(clean)\n",
    "    # replace 𝓕𝔯𝔞𝔫𝔠𝔢𝔰𝔠𝔞 by Francesca\n",
    "    clean = unicodedata.normalize(\"NFKC\", clean)\n",
    "    return clean\n",
    "\n",
    "def get_batch_pairs(batch_size, dataset=\"train\", max_chars=None, apply_variations=True):\n",
    "    # Calculate weights based on dataset sizes\n",
    "    weights = []\n",
    "    for corp in corpus_objects:\n",
    "        if dataset == \"train\":\n",
    "            weights.append(len(corp.df_train))\n",
    "        elif dataset == \"validate\":\n",
    "            weights.append(len(corp.df_validate))\n",
    "        elif dataset == \"test\":\n",
    "            weights.append(len(corp.df_test))\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid dataset specified: {dataset}. Choose from 'train', 'validate', or 'test'.\")\n",
    "    \n",
    "    weights = [w / sum(weights) for w in weights]\n",
    "    corpus = np.random.choice(corpus_objects, p=weights, replace=False)\n",
    "    \n",
    "    # Sample the batch\n",
    "    if dataset == \"train\":\n",
    "        batch = corpus.df_train.sample(n=batch_size)\n",
    "    elif dataset == \"validate\":\n",
    "        batch = corpus.df_validate.sample(n=batch_size)\n",
    "    elif dataset == \"test\":\n",
    "        batch = corpus.df_test.sample(n=batch_size)\n",
    "    \n",
    "    # Preprocess sentences\n",
    "    batch['source_sentence'] = batch['source_sentence'].apply(preproc)\n",
    "    batch['target_sentence'] = batch['target_sentence'].apply(preproc)\n",
    "    \n",
    "    source_lang_long = corpus.source_lang_long\n",
    "    target_lang_long = corpus.target_lang_long\n",
    "    \n",
    "    xx = batch['source_sentence'].tolist()\n",
    "    yy = batch['target_sentence'].tolist()\n",
    "    \n",
    "    # Optional: Apply variations\n",
    "    if apply_variations:\n",
    "        xx, yy = add_data_variations(xx, yy, source_lang_long, target_lang_long, batch_size)\n",
    "    \n",
    "    # Trim sentences if max_chars is specified\n",
    "    if max_chars:\n",
    "        def truncate_at_space(sent, max_len):\n",
    "            if len(sent) <= max_len:\n",
    "                return sent\n",
    "            # Find the last space before max_len\n",
    "            truncate = sent[:max_len]\n",
    "            return truncate[:truncate.rfind(\" \")]\n",
    "        xx = [truncate_at_space(x, max_chars) for x in xx]\n",
    "        yy = [truncate_at_space(y, max_chars) for y in yy]\n",
    "    \n",
    "    return xx, yy, source_lang_long, target_lang_long\n",
    "\n",
    "def add_gronings_variations(sentences):\n",
    "    # Gronings-specific removal of more or less optional diacritics\n",
    "    if not random.getrandbits(2):\n",
    "        sentences = [s.replace('ì', 'i').replace('è', 'e').replace('ò', 'o').replace('ó', 'o') for s in sentences]\n",
    "    return sentences\n",
    "\n",
    "# List of synonym pairs\n",
    "synonym_pairs_gos = [\n",
    "    ('huus', 'hoes'), ('huzen', 'hoezen'), ('huuske', 'hoeske'), ('groag', 'geern'), ('raais', 'raaize'), ('kees', 'keze'), ('week', 'weke'),\n",
    "    ('mìnsken', 'mìnsen'), ('uut', 'oet'), ('in', 'ien'), ('wer', 'wuir'), ('gebruuk', 'gebroek'), ('zuch', 'zok'), ('bruukst', 'broekst'), ('wind', 'wiend'),\n",
    "    ('vanuut', 'vanoet'), ('wazzen', 'waren'), ('mekoar', 'nkander'), ('bruken', 'broeken'), ('zuch', 'zuk'), ('vis', 'visk'), ('olle', 'olde'),\n",
    "    ('zuk', 'zok'), ('wotter', 'woater'), ('kraant', 'kraande'), ('haar', 'har'), ('bruuk', 'broek'), ('school', 'schoule'), ('iezer', 'iesder'),\n",
    "    ('ais', 'ains'), ('hebben', 'hemmen'), ('zotterdag', 'zoaterdag'), ('bruukt', 'broekt'), ('bruukten', 'broekten'), ('iezern', 'iesdern'), ('kind', 'kiend'),\n",
    "    ('mirreg', 'middag'), ('vast', 'vaast'), ('nacht', 'naacht'), ('kiender', 'kinder'), ('bruukte', 'broekte'), ('deus','deuze'), ('gelok', 'geluk')\n",
    "]\n",
    "\n",
    "def swap_synonyms(sentences, synonym_pairs, swap_prob=0.25):\n",
    "    swapped_sentences = []\n",
    "\n",
    "    for sent in sentences:\n",
    "        words = sent.split()  # Split sentence into words\n",
    "        new_words = []\n",
    "        \n",
    "        for word in words:\n",
    "            replaced = False\n",
    "            \n",
    "            # Check each synonym pair\n",
    "            for word1, word2 in synonym_pairs:\n",
    "                if word == word1 and random.random() < swap_prob:\n",
    "                    # print(sent)\n",
    "                    new_words.append(word2)  # Replace with the second variant\n",
    "                    replaced = True\n",
    "                    break\n",
    "                elif word == word2 and random.random() < swap_prob:\n",
    "                    new_words.append(word1)  # Replace with the first variant\n",
    "                    replaced = True\n",
    "                    break\n",
    "            \n",
    "            if not replaced:\n",
    "                new_words.append(word)  # Keep the original word if not replaced\n",
    "        \n",
    "        # Reconstruct the sentence\n",
    "        swapped_sentences.append(' '.join(new_words))\n",
    "    \n",
    "    return swapped_sentences\n",
    "\n",
    "\n",
    "def add_data_variations(xx, yy, source_lang_long, target_lang_long, batch_size):\n",
    "    # Randomly swap source and target languages\n",
    "    if random.getrandbits(1):\n",
    "        xx, yy, source_lang_long, target_lang_long = yy, xx, target_lang_long, source_lang_long\n",
    "\n",
    "    if target_lang_long == 'gos_Latn':\n",
    "        yy = add_gronings_variations(yy)\n",
    "        yy = swap_synonyms(yy, synonym_pairs_gos)\n",
    "    elif source_lang_long == 'gos_Latn':\n",
    "        xx = add_gronings_variations(xx)\n",
    "        xx = swap_synonyms(xx, synonym_pairs_gos)\n",
    "\n",
    "    # Create more name variation (e.g., replacing \"Tom\")\n",
    "    for i in range(len(xx)):\n",
    "        if \"Tom\" in xx[i] and \"Tom\" in yy[i]:\n",
    "            namelist = ['Tim', 'Sam', 'Ben', 'Nick', 'Ed', 'Noah', 'Joey', 'Rick', 'Rob', 'Mick', 'Mike', 'Michael', 'Tom', 'Adam', 'Arnold', 'Lucas', 'Robin', 'James', 'Jim']\n",
    "            othername = random.choice(namelist)\n",
    "            xx[i] = xx[i].replace(\"Tom\", othername)\n",
    "            yy[i] = yy[i].replace(\"Tom\", othername)\n",
    "\n",
    "    # Small chance of uppercase transformation\n",
    "    if not random.getrandbits(5):\n",
    "        xx = [x.upper() for x in xx]\n",
    "        yy = [y.upper() for y in yy]\n",
    "\n",
    "    # chance of no capitalization at sentence start\n",
    "    elif not random.getrandbits(3):\n",
    "        xx = [x[:1].lower() + x[1:] for x in xx]\n",
    "        yy = [y[:1].lower() + y[1:] for y in yy]\n",
    "\n",
    "    # Small chance of random emoji at the end\n",
    "    if not random.getrandbits(3):\n",
    "        emojis = random.choices([\"😊\", \"😂\", \"😍\", \"👍\", \"🔥\", \"🎉\", \"🌟\", \"😎\", \"🥳\", '❤️', '💀', '😭', '🫶', '🤣', '😘', '🥺', '🤔', '🙏'], k=batch_size)\n",
    "        xx = [xx[i] + emojis[i] for i in range(batch_size)]\n",
    "        yy = [yy[i] + emojis[i] for i in range(batch_size)]\n",
    "\n",
    "    # Small chance of sentence-final character deletion\n",
    "    elif not random.getrandbits(3):\n",
    "        xx = [x[:-1] if len(x) > 1 else x for x in xx]\n",
    "        yy = [y[:-1] if len(y) > 1 else y for y in yy]\n",
    "\n",
    "    return xx, yy\n",
    "\n",
    "get_batch_pairs(20, dataset='train', max_chars=220);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1BV9mcZwmLd"
   },
   "source": [
    "## 6. The training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.train()\n",
    "# x, y, loss = None, None, None\n",
    "# cleanup()\n",
    "\n",
    "# max_length = 90\n",
    "# max_chars = 120\n",
    "# batch_size = 20\n",
    "# xx, yy, lang1, lang2 = get_batch_pairs(batch_size)#, max_chars=max_chars)\n",
    "\n",
    "# tokenizer.src_lang = lang1\n",
    "# x = tokenizer(xx, return_tensors='pt', padding='longest', truncation=True, max_length=max_length).to(model.device) # max length is a number of tokens.........\n",
    "# tokenizer.src_lang = lang2\n",
    "# y = tokenizer(yy, return_tensors='pt', padding='longest', truncation=True, max_length=max_length).to(model.device)\n",
    "# y.input_ids[y.input_ids == tokenizer.pad_token_id] = -100\n",
    "\n",
    "# loss = model(**x, labels=y.input_ids).loss\n",
    "# # loss.backward()\n",
    "# # losses.append(loss.item())\n",
    "\n",
    "# # optimizer.step()\n",
    "# # optimizer.zero_grad(set_to_none=True)\n",
    "# # scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 304,
     "referenced_widgets": [
      "a7333450367f4d9b889827ca684618ba",
      "f10c62ba1c0d4a8abb5e2ac9ebb1b597",
      "bafb9ac089624cbe856f7e915ff2e33d",
      "70c2984da31e41f997de57d4d7c296b9",
      "f72f5732980148f3bf389e0d55077a69",
      "2430c208c59843fb81ab33724c2a06ff",
      "96145ae9b0f34c4abda7087504780826",
      "129499bfe1db45f3b6423f37d5196086",
      "057d5ee247d54cc486cc9266e562f1db",
      "10100514800a434f94dab81dc7e8126a",
      "843ab819836c400eb482b07d03f02209"
     ]
    },
    "id": "ahPBT-vt_c91",
    "outputId": "d545fe57-3d5e-418b-a92b-3cd58c428db2"
   },
   "outputs": [],
   "source": [
    "model.train()\n",
    "x, y, loss = None, None, None\n",
    "cleanup()\n",
    "\n",
    "tq = trange(len(losses), training_steps)\n",
    "for i in tq:\n",
    "    xx, yy, lang1, lang2 = get_batch_pairs(batch_size, max_chars=max_chars)\n",
    "    try:\n",
    "        tokenizer.src_lang = lang1\n",
    "        x = tokenizer(xx, return_tensors='pt', padding='longest', truncation=True, max_length=max_length).to(model.device)\n",
    "        tokenizer.src_lang = lang2\n",
    "        y = tokenizer(yy, return_tensors='pt', padding='longest', truncation=True, max_length=max_length).to(model.device)\n",
    "        y.input_ids[y.input_ids == tokenizer.pad_token_id] = -100\n",
    "\n",
    "        loss = model(**x, labels=y.input_ids).loss\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        scheduler.step()\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        x, y, loss = None, None, None\n",
    "        cleanup()\n",
    "        print('error', max(len(s) for s in xx + yy), e)\n",
    "        continue\n",
    "\n",
    "    if i % 25 == 0 and i > 0:\n",
    "        cleanup()\n",
    "        print(i, \"mean loss of last 25 steps:\\t\", np.mean(losses[-25:]))\n",
    "        if i > 1*training_steps/4:\n",
    "            model.save_pretrained(MODEL_SAVE_PATH+str(i))\n",
    "            tokenizer.save_pretrained(MODEL_SAVE_PATH+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should in near future start saving hyperparameters, something like this:\n",
    "# # Function to save a model\n",
    "# def save_model(model, hyperparameters):\n",
    "#     timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "#     model_folder = f\"{MODEL_SAVE_PATH}{timestamp}\"\n",
    "    \n",
    "#     # Create the folder\n",
    "#     os.makedirs(model_folder, exist_ok=True)\n",
    "    \n",
    "#     # Save the model (this is a placeholder, replace it with actual saving code)\n",
    "#     model.save(os.path.join(model_folder, 'model.pth'))\n",
    "    \n",
    "#     # Save the hyperparameters to a text file\n",
    "#     with open(os.path.join(model_folder, 'hyperparameters.txt'), 'w') as f:\n",
    "#         for key, value in hyperparameters.items():\n",
    "#             f.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "# # Example usage\n",
    "# model = None  # Replace with your actual model\n",
    "# hyperparameters = {'learning_rate': 0.001, 'batch_size': 32}  # Replace with actual hyperparameters\n",
    "# save_model(model, hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(MODEL_SAVE_PATH+str(training_steps))\n",
    "tokenizer.save_pretrained(MODEL_SAVE_PATH+str(training_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "xXXT9pcd_9Au",
    "outputId": "58658ffc-f3d0-4a85-8884-cdca6ba08e17"
   },
   "outputs": [],
   "source": [
    "pd.Series(losses).ewm(30).mean().plot();\n",
    "pd.Series(losses).ewm(100).mean().plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6MGVf4Vc_fS4"
   },
   "outputs": [],
   "source": [
    "def translate(text, src_lang='nld_Latn', tgt_lang='gos_Latn', a=16, b=1.5, max_input_length=700, **kwargs):\n",
    "    tokenizer.src_lang = src_lang\n",
    "    tokenizer.tgt_lang = tgt_lang\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=max_input_length)\n",
    "    result = model.generate(\n",
    "        **inputs.to(model.device),\n",
    "        forced_bos_token_id=tokenizer.convert_tokens_to_ids(tgt_lang),\n",
    "        max_new_tokens=int(a + b * inputs.input_ids.shape[1]),\n",
    "        **kwargs\n",
    "    )\n",
    "    #print(inputs.input_ids.shape[1], result.shape[1])\n",
    "    return tokenizer.batch_decode(result, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'De wetenschappers zijn bijna klaar om hun bevindingen te publiceren. Dat zal nog ongeveer een week of dertien kunnen duren. Maar als de resultaten kloppen, zou het universum tien miljard jaar nodig hebben om planeten te vormen. De aarde is nu 4 1⁄2 miljard jaar oud en het universum 15 miljard jaar oud. We zouden dus tot de eerste generatie planeten en überhaupt tot het eerste intelligente leven behoren! Dat vind ik ontzettend indrukwekkend en het houdt me s nachts wakker. De hele nacht lig ik erover te piekeren en vind ik het zo ontzettend bijzonder.'\n",
    "langA = 'nld_Latn'\n",
    "langB = 'gos_Latn'\n",
    "# note it looks like translation between any pairs that don't contain gronings is broken..\n",
    "print(s)\n",
    "t = translate(s, langA, langB)\n",
    "print(t)\n",
    "t_back = translate(t, langB, langA)\n",
    "print(t_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c69XqtpbAgjN",
    "outputId": "2b963659-10e1-4cfc-fe20-ef136aef75e8"
   },
   "outputs": [],
   "source": [
    "from sacrebleu import corpus_bleu, corpus_chrf\n",
    "\n",
    "# Evaluate translations for validation and training data\n",
    "results = []\n",
    "\n",
    "for corpus in tqdm(corpus_objects, desc=\"Processing corpora\"):\n",
    "    # cleanup()\n",
    "    # Access validation and training data\n",
    "    df_validate = corpus.df_validate.sample(n=min(len(corpus.df_validate), 200), random_state=9358)\n",
    "    df_train_sample = corpus.df_train.sample(n=max([len(df_validate), 200]), random_state=9358)\n",
    "    \n",
    "    for dataset_name, dataset in [(\"validate\", df_validate), (\"train\", df_train_sample)]:\n",
    "        # Prepare data for translation\n",
    "        src_sentences = dataset['source_sentence'].tolist()\n",
    "        tgt_sentences = dataset['target_sentence'].tolist()\n",
    "        \n",
    "        # Translate source to target\n",
    "        translations_src_to_tgt = translate(\n",
    "            text=src_sentences,\n",
    "            src_lang=corpus.source_lang_long,\n",
    "            tgt_lang=corpus.target_lang_long\n",
    "        )\n",
    "        \n",
    "        # Translate target to source\n",
    "        translations_tgt_to_src = translate(\n",
    "            text=tgt_sentences,\n",
    "            src_lang=corpus.target_lang_long,\n",
    "            tgt_lang=corpus.source_lang_long\n",
    "        )\n",
    "        \n",
    "        # Calculate metrics\n",
    "        bleu_src_to_tgt = corpus_bleu(tgt_sentences, [translations_src_to_tgt]).score\n",
    "        bleu_tgt_to_src = corpus_bleu(src_sentences, [translations_tgt_to_src]).score\n",
    "        \n",
    "        chrf_src_to_tgt = corpus_chrf(tgt_sentences, [translations_src_to_tgt]).score\n",
    "        chrf_tgt_to_src = corpus_chrf(src_sentences, [translations_tgt_to_src]).score\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            \"source_lang\": corpus.source_lang,\n",
    "            \"target_lang\": corpus.target_lang,\n",
    "            \"dataset\": dataset_name,\n",
    "            \"bleu_src_to_tgt\": bleu_src_to_tgt,\n",
    "            \"bleu_tgt_to_src\": bleu_tgt_to_src,\n",
    "            \"chrf_src_to_tgt\": chrf_src_to_tgt,\n",
    "            \"chrf_tgt_to_src\": chrf_tgt_to_src,\n",
    "        })\n",
    "\n",
    "# Convert results to a DataFrame for easier analysis\n",
    "df_results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.precision', 1)\n",
    "display(df_results)\n",
    "\n",
    "#####\n",
    "# After adapting code to sample languages weighted by corpus size, Dutch results look balanced. English seems to still overfit, German probably too.\n",
    "# Scores from Gronings to the other languages are often higher than the other way around. Don't think it's surprising with a multilingual model.\n",
    "# A very multilingual model containing Dutch English German Spanish Afrikaans and Danish seems to perform worse on Dutch. But surprisingly good on Danish??? Or maybe that's an artifact of too small validation set.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dutch scores for training on English and German as well:\n",
    "```\n",
    "\tsource_lang\ttarget_lang\tdataset\t\tbleu_src_to_tgt\tbleu_tgt_to_src\tchrf_src_to_tgt\tchrf_tgt_to_src\n",
    "0\tnld\t\tgos\t\tvalidate\t38.9\t\t60.4\t\t27.8\t\t24.3\n",
    "1\tnld\t\tgos\t\ttrain\t\t37.3\t\t60.8\t\t22.3\t\t22.0\n",
    "```\n",
    "\n",
    "Scores for just a nld-gos model\n",
    "tldr all scores are significantly higher. gos to nld bleu improved the least"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's try to investigate overfitting.\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import typing as tp\n",
    "import unicodedata\n",
    "from transformers import AutoModelForSeq2SeqLM, NllbTokenizer\n",
    "from sacremoses import MosesPunctNormalizer\n",
    "from sacrebleu import corpus_bleu, corpus_chrf\n",
    "\n",
    "# Preprocessing setup\n",
    "mpn = MosesPunctNormalizer(lang=\"en\")\n",
    "mpn.substitutions = [\n",
    "    (re.compile(r), sub) for r, sub in mpn.substitutions\n",
    "]\n",
    "\n",
    "def get_non_printing_char_replacer(replace_by: str = \" \") -> tp.Callable[[str], str]:\n",
    "    non_printable_map = {\n",
    "        ord(c): replace_by\n",
    "        for c in (chr(i) for i in range(sys.maxunicode + 1))\n",
    "        # same as \\p{C} in perl\n",
    "        # see https://www.unicode.org/reports/tr44/#General_Category_Values\n",
    "        if unicodedata.category(c) in {\"C\", \"Cc\", \"Cf\", \"Cs\", \"Co\", \"Cn\"}\n",
    "    }\n",
    "\n",
    "    def replace_non_printing_char(line) -> str:\n",
    "        return line.translate(non_printable_map)\n",
    "\n",
    "    return replace_non_printing_char\n",
    "\n",
    "replace_nonprint = get_non_printing_char_replacer(\" \")\n",
    "\n",
    "def preproc(text):\n",
    "    clean = mpn.normalize(text)\n",
    "    clean = replace_nonprint(clean)\n",
    "    clean = unicodedata.normalize(\"NFKC\", clean)  # Normalize fancy characters\n",
    "    return clean\n",
    "\n",
    "# Function to translate with preprocessing\n",
    "def translate(text, src_lang, tgt_lang, model, tokenizer):\n",
    "    # Apply preprocessing to the text\n",
    "    preprocessed_text = [preproc(sentence) for sentence in text]\n",
    "\n",
    "    tokenizer.src_lang = src_lang\n",
    "    inputs = tokenizer(preprocessed_text, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "    outputs = model.generate(**inputs)\n",
    "    return [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "def fix_tokenizer(tokenizer, new_lang='gos_Latn'):\n",
    "    \"\"\"\n",
    "    Add a new language token to the tokenizer vocabulary\n",
    "    (this should be done each time after its initialization)\n",
    "    \"\"\"\n",
    "    old_len = len(tokenizer) - int(new_lang in tokenizer.added_tokens_encoder)\n",
    "    tokenizer.lang_code_to_id[new_lang] = old_len-1\n",
    "    tokenizer.id_to_lang_code[old_len-1] = new_lang\n",
    "    # always move \"mask\" to the last position\n",
    "    tokenizer.fairseq_tokens_to_ids[\"<mask>\"] = len(tokenizer.sp_model) + len(tokenizer.lang_code_to_id) + tokenizer.fairseq_offset\n",
    "\n",
    "    tokenizer.fairseq_tokens_to_ids.update(tokenizer.lang_code_to_id)\n",
    "    tokenizer.fairseq_ids_to_tokens = {v: k for k, v in tokenizer.fairseq_tokens_to_ids.items()}\n",
    "    if new_lang not in tokenizer._additional_special_tokens:\n",
    "        tokenizer._additional_special_tokens.append(new_lang)\n",
    "    # clear the added token encoder; otherwise a new token may end up there by mistake\n",
    "    tokenizer.added_tokens_encoder = {}\n",
    "    tokenizer.added_tokens_decoder = {}\n",
    "\n",
    "# Model loading and evaluation\n",
    "def load_and_evaluate_model(version_path, corpus_objects):\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(version_path).cuda()\n",
    "    tokenizer = NllbTokenizer.from_pretrained(version_path)\n",
    "\n",
    "    fix_tokenizer(tokenizer)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    results = []\n",
    "    for corpus in corpus_objects:\n",
    "        # Access validation data\n",
    "        df_validate = corpus.df_validate.sample(n=min(len(corpus.df_validate), 200), random_state=9358)\n",
    "        src_sentences = df_validate['source_sentence'].tolist()\n",
    "        tgt_sentences = df_validate['target_sentence'].tolist()\n",
    "\n",
    "        # Translate with preprocessing\n",
    "        translations_src_to_tgt = translate(\n",
    "            text=src_sentences,\n",
    "            src_lang=corpus.source_lang_long,\n",
    "            tgt_lang=corpus.target_lang_long,\n",
    "            model=model,\n",
    "            tokenizer=tokenizer\n",
    "        )\n",
    "\n",
    "        translations_tgt_to_src = translate(\n",
    "            text=tgt_sentences,\n",
    "            src_lang=corpus.target_lang_long,\n",
    "            tgt_lang=corpus.source_lang_long,\n",
    "            model=model,\n",
    "            tokenizer=tokenizer\n",
    "        )\n",
    "\n",
    "        # Calculate BLEU and CHRF metrics\n",
    "        bleu_src_to_tgt = corpus_bleu(tgt_sentences, [translations_src_to_tgt]).score\n",
    "        bleu_tgt_to_src = corpus_bleu(src_sentences, [translations_tgt_to_src]).score\n",
    "\n",
    "        chrf_src_to_tgt = corpus_chrf(tgt_sentences, [translations_src_to_tgt]).score\n",
    "        chrf_tgt_to_src = corpus_chrf(src_sentences, [translations_tgt_to_src]).score\n",
    "\n",
    "        results.append({\n",
    "            \"bleu_src_to_tgt\": bleu_src_to_tgt,\n",
    "            \"bleu_tgt_to_src\": bleu_tgt_to_src,\n",
    "            \"chrf_src_to_tgt\": chrf_src_to_tgt,\n",
    "            \"chrf_tgt_to_src\": chrf_tgt_to_src,\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "import os\n",
    "\n",
    "# Define the base directory containing the model folders\n",
    "MODEL_BASE_PATH = '../NLLB-GOS/models/'\n",
    "\n",
    "# Filter for directories containing \"MODEL_BASE_PATH\" in their name\n",
    "model_versions = [folder_name for folder_name in os.listdir(MODEL_BASE_PATH) if MODEL_SAVE_PATH[len(MODEL_BASE_PATH):] in folder_name]\n",
    "\n",
    "for path in model_versions:\n",
    "    print(path)\n",
    "\n",
    "# Evaluate all versions\n",
    "all_results = {}\n",
    "for model_name in model_versions:\n",
    "    if MODEL_BASE_PATH+model_name != MODEL_SAVE_PATH:\n",
    "        step = model_name[-3:]\n",
    "    else:\n",
    "        print(model_name,'!')\n",
    "        step = training_steps\n",
    "    print(f\"Evaluating model saved at step {step}...\")\n",
    "    version_results = load_and_evaluate_model(MODEL_BASE_PATH+model_name, corpus_objects)\n",
    "    \n",
    "    avg_results = pd.DataFrame(version_results).mean().to_dict()  # Average scores\n",
    "    all_results[step] = avg_results\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "df_results = pd.DataFrame.from_dict(all_results, orient=\"index\")\n",
    "df_results.index.name = \"Training Steps\"\n",
    "df_results.reset_index(inplace=True)\n",
    "df_results.to_csv(f'overfitting investigation {timestamp}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 6))\n",
    "metrics = [\"bleu_src_to_tgt\", \"bleu_tgt_to_src\"]\n",
    "for metric in metrics:\n",
    "    plt.plot(df_results[\"Training Steps\"], df_results[metric], label=metric)\n",
    "\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Model Performance (bleu) by Training Step\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "metrics = [\"chrf_src_to_tgt\", \"chrf_tgt_to_src\"]\n",
    "for metric in metrics:\n",
    "    plt.plot(df_results[\"Training Steps\"], df_results[metric], label=metric)\n",
    "\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Model Performance (chrf) by Training Step\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0qubmjZNAxJB"
   },
   "source": [
    "## 7. Using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PKGZ8zuN2mV6"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import NllbTokenizer, AutoModelForSeq2SeqLM, AutoConfig\n",
    "from tqdm.auto import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0AXtm-Qf2wCR"
   },
   "outputs": [],
   "source": [
    "# this code is adapted from the Stopes repo of the NLLB team\n",
    "# https://github.com/facebookresearch/stopes/blob/main/stopes/pipelines/monolingual/monolingual_line_processor.py#L214\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import typing as tp\n",
    "import unicodedata\n",
    "from sacremoses import MosesPunctNormalizer\n",
    "\n",
    "\n",
    "mpn = MosesPunctNormalizer(lang=\"en\")\n",
    "mpn.substitutions = [\n",
    "    (re.compile(r), sub) for r, sub in mpn.substitutions\n",
    "]\n",
    "\n",
    "\n",
    "def get_non_printing_char_replacer(replace_by: str = \" \") -> tp.Callable[[str], str]:\n",
    "    non_printable_map = {\n",
    "        ord(c): replace_by\n",
    "        for c in (chr(i) for i in range(sys.maxunicode + 1))\n",
    "        # same as \\p{C} in perl\n",
    "        # see https://www.unicode.org/reports/tr44/#General_Category_Values\n",
    "        if unicodedata.category(c) in {\"C\", \"Cc\", \"Cf\", \"Cs\", \"Co\", \"Cn\"}\n",
    "    }\n",
    "\n",
    "    def replace_non_printing_char(line) -> str:\n",
    "        return line.translate(non_printable_map)\n",
    "\n",
    "    return replace_non_printing_char\n",
    "\n",
    "replace_nonprint = get_non_printing_char_replacer(\" \")\n",
    "\n",
    "def preproc(text):\n",
    "    clean = mpn.normalize(text)\n",
    "    clean = replace_nonprint(clean)\n",
    "    # replace 𝓕𝔯𝔞𝔫𝔠𝔢𝔰𝔠𝔞 by Francesca\n",
    "    clean = unicodedata.normalize(\"NFKC\", clean)\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wwb6ck8P25ZQ"
   },
   "outputs": [],
   "source": [
    "model_load_name = MODEL_SAVE_PATH\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_load_name).cuda()\n",
    "tokenizer = NllbTokenizer.from_pretrained(model_load_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uY7nUGsX3NOM",
    "outputId": "84976f43-9775-443d-ba5e-7da564be2ed4"
   },
   "outputs": [],
   "source": [
    "def fix_tokenizer(tokenizer, new_lang='gos_Latn'):\n",
    "    \"\"\"\n",
    "    Add a new language token to the tokenizer vocabulary\n",
    "    (this should be done each time after its initialization)\n",
    "    \"\"\"\n",
    "    old_len = len(tokenizer) - int(new_lang in tokenizer.added_tokens_encoder)\n",
    "    tokenizer.lang_code_to_id[new_lang] = old_len-1\n",
    "    tokenizer.id_to_lang_code[old_len-1] = new_lang\n",
    "    # always move \"mask\" to the last position\n",
    "    tokenizer.fairseq_tokens_to_ids[\"<mask>\"] = len(tokenizer.sp_model) + len(tokenizer.lang_code_to_id) + tokenizer.fairseq_offset\n",
    "\n",
    "    tokenizer.fairseq_tokens_to_ids.update(tokenizer.lang_code_to_id)\n",
    "    tokenizer.fairseq_ids_to_tokens = {v: k for k, v in tokenizer.fairseq_tokens_to_ids.items()}\n",
    "    if new_lang not in tokenizer._additional_special_tokens:\n",
    "        tokenizer._additional_special_tokens.append(new_lang)\n",
    "    # clear the added token encoder; otherwise a new token may end up there by mistake\n",
    "    tokenizer.added_tokens_encoder = {}\n",
    "    tokenizer.added_tokens_decoder = {}\n",
    "\n",
    "fix_tokenizer(tokenizer)\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZIsPI6YT3UG0"
   },
   "outputs": [],
   "source": [
    "def translate(text, src_lang='nld_Latn', tgt_lang='gos_Latn', a=16, b=1.5, max_input_length=700, **kwargs):\n",
    "    tokenizer.src_lang = src_lang\n",
    "    tokenizer.tgt_lang = tgt_lang\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=max_input_length)\n",
    "    result = model.generate(\n",
    "        **inputs.to(model.device),\n",
    "        forced_bos_token_id=tokenizer.convert_tokens_to_ids(tgt_lang),\n",
    "        max_new_tokens=int(a + b * inputs.input_ids.shape[1]),\n",
    "        **kwargs\n",
    "    )\n",
    "    #print(inputs.input_ids.shape[1], result.shape[1])\n",
    "    return tokenizer.batch_decode(result, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UJwLBH8M9XWW",
    "outputId": "8cd3007f-6b6e-4364-ca99-991efe0d719e"
   },
   "outputs": [],
   "source": [
    "t = 'wie springen op en del.'\n",
    "print(translate(t, 'gos_Latn', 'nld_Latn'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o9JFXvBS9xY7",
    "outputId": "09a8e62c-d727-4f72-8915-bed8a0e4498c"
   },
   "outputs": [],
   "source": [
    "translate(t, 'nld_Latn', 'gos_Latn', do_sample=True, num_beams=2, temperature=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gosdemo = pd.read_csv('../gos-kdl/texts.csv')\n",
    "target_lang = \"nld_Latn\"\n",
    "batch_size = 16\n",
    "translations = []\n",
    "for i in tqdm(range(0, len(gosdemo), batch_size), desc=\"Translating batches\"):\n",
    "    batch = gosdemo[\"text\"].tolist()[i:i + batch_size]\n",
    "    translations.extend(translate(batch, src_lang=\"gos_Latn\", tgt_lang=target_lang))\n",
    "\n",
    "gosdemo[f\"translation_to_{target_lang}\"] = translations\n",
    "\n",
    "# for d in ['ö','ò','ó','ô','è','ì','ë','wui ']:\n",
    "#     # print(d)\n",
    "#     for i in gosdemo.iterrows():\n",
    "#         # print(i)\n",
    "#         for txt in gosdemo['text']:\n",
    "#             if d in txt:\n",
    "#                 print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "display(gosdemo[['text','translation_to_nld_Latn']].sample(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "3a2c606d8688424aa3b325223cfc1f08",
      "5ddb689b97db48aaa93001a91b86a4a2",
      "01d711c111174832ab5bba94cfbca2a6",
      "b3aeb847f0374c2ba47ec22c60ab2f18",
      "b7466c53e33c4b23b9186821609eefa1",
      "fc8279d880854b5bb526cadc437bc317",
      "4b9195f13aa34a87a2859654902ef99a",
      "804c3b981cc740b79f700e7cecf9362e",
      "4ef9639ade9e49189272e6b8012223e8",
      "f87dfba0b04e42cabd525af31afae2ca",
      "a82b144e2c024fc8822def3701384c12",
      "0ea7235036c14d438a412ae6fe857ab0",
      "7b7348cd7da243158c0734488b422b74",
      "4aaad174030140a69db1e81361553902",
      "8d6093633eb44e2c9b499a1594e2b6dc",
      "2beeb8b43c3c4153b7723184ec6ab123",
      "2548884655614dfda0d263cacd665c07",
      "70099123340b4adca9f18dcd23cbb270",
      "f3782ed9cd074d43a69e2978eb7b0865",
      "3feb136a616d46b7a0100d2a507cbd95",
      "b9dfdf81307c4615b00ba80b3eef2958",
      "2ea8b05ab5f540efa609befac402a7d1"
     ]
    },
    "id": "k3S475uG3fBh",
    "outputId": "abacfc33-ac7e-4471-c5f1-e1a5c9f8aabc"
   },
   "outputs": [],
   "source": [
    "df_validate['nld_translated'] = [translate(t, 'gos_Latn', 'nld_Latn')[0] for t in tqdm(df_validate.gos)]\n",
    "df_validate['gos_translated'] = [translate(t, 'nld_Latn', 'gos_Latn')[0] for t in tqdm(df_validate.nld)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validate.sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FMRSCWW732ya"
   },
   "outputs": [],
   "source": [
    "import sacrebleu\n",
    "bleu_calc = sacrebleu.BLEU()\n",
    "chrf_calc = sacrebleu.CHRF(word_order=2)  # this metric is called ChrF++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9NEfm2fmJm1S",
    "outputId": "e41a7dd6-4b67-4bcc-ba9b-61b01f319631"
   },
   "outputs": [],
   "source": [
    "!pip install editdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NxxFrdC7JrS6",
    "outputId": "52a5e77e-34e4-4733-aead-6f83d14be049"
   },
   "outputs": [],
   "source": [
    "import editdistance\n",
    "\n",
    "def ed_similarity(text1, text2):\n",
    "    return max(0, 1 - editdistance.eval(text1, text2) / min(len(text1), len(text2)))\n",
    "\n",
    "print(ed_similarity('кот', 'собака'))\n",
    "print(ed_similarity('кот', 'кит'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EjkvAyN9JyXg",
    "outputId": "6df153cb-e719-4bbf-d7aa-78311cfdb05c"
   },
   "outputs": [],
   "source": [
    "display(pd.Series([ed_similarity(row.nld, row.nld_translated) for row in df_validate.itertuples()]).describe())\n",
    "display(pd.Series([ed_similarity(row.gos, row.gos_translated) for row in df_validate.itertuples()]).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R5YCOHIw7TgW",
    "outputId": "35e04aac-d92f-4775-9655-49580721e8b2"
   },
   "outputs": [],
   "source": [
    "print(bleu_calc.corpus_score(df_validate['rus_translated2'].tolist(), [df_validate['ru'].tolist()]))\n",
    "print(chrf_calc.corpus_score(df_validate['rus_translated2'].tolist(), [df_validate['ru'].tolist()]))\n",
    "print(bleu_calc.corpus_score(df_validate['tyv_translated2'].tolist(), [df_validate['tyv'].tolist()]))\n",
    "print(chrf_calc.corpus_score(df_validate['tyv_translated2'].tolist(), [df_validate['tyv'].tolist()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "3417c70a8db54723804a9de92094eec6",
      "39fbccba9def4fce9e41edf0f931e667",
      "1555767cc3334aa185368367f27b49a6",
      "637865b7608e4aae82b8d87235d8172e",
      "7251bc0a86ce4739b512c814941eab7d",
      "9c2a4c33dc1d4eb2adf449272e1c1d7e",
      "db15b4bda74441d58fde31d2287c8cbe",
      "33daa47f93874045a29f7d77e5a9d7fc",
      "ed443ff78a184cc49632b65197010277",
      "53aefd2209db419181b816792cacfdd8",
      "025040b9a9e54109bc20ebd217bfd74c",
      "24f0dfcbe8b144828b5efb5d387c15d5",
      "39a4ac85caa7443f8348c2e2c68b64f2",
      "b2c8f8f89565450c85bf02e80cfadc39",
      "6f44ede97c0b4ca79dcbea7bb31402a9",
      "5558af48295945cb88aaefedc1f25ed4",
      "5e371b1b259248b1bb34ba66f9febdf5",
      "7009b83f25c147c8b173886f51ba21d1",
      "b1de5494ad734e44934ede08df117937",
      "002a75bfa1c943aca2e5c2ec06ad4900",
      "65d56cb8b3584d64b94f6194f76d0f4e",
      "f64c0ee1822f45c1a24f18765324cdd5"
     ]
    },
    "id": "GPU1GA1HDJ9h",
    "outputId": "c00c2fbd-c456-4660-d7fd-ee83e729e601"
   },
   "outputs": [],
   "source": [
    "df_validate['rus2eng'] = [translate(t, 'tyv_Cyrl', 'eng_Latn')[0] for t in tqdm(df_validate.tyv)]\n",
    "df_validate['tyv2eng'] = [translate(t, 'rus_Cyrl', 'eng_Latn')[0] for t in tqdm(df_validate.ru)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ux7sF9Cv7_o1"
   },
   "source": [
    "Results with num_beams=1:\n",
    "```\n",
    "V1\n",
    "BLEU = 23.21 51.2/29.1/18.0/11.8 (BP = 0.978 ratio = 0.978 hyp_len = 2273 ref_len = 2324)\n",
    "chrF2++ = 47.88\n",
    "BLEU = 22.03 51.5/29.7/17.9/10.4 (BP = 0.952 ratio = 0.953 hyp_len = 2260 ref_len = 2371)\n",
    "chrF2++ = 49.37\n",
    "V2\n",
    "BLEU = 24.08 50.9/29.5/19.1/12.3 (BP = 0.988 ratio = 0.988 hyp_len = 2297 ref_len = 2324)\n",
    "chrF2++ = 48.96\n",
    "BLEU = 22.50 50.5/28.5/17.7/11.1 (BP = 0.974 ratio = 0.974 hyp_len = 2310 ref_len = 2371)\n",
    "chrF2++ = 48.85\n",
    "V3\n",
    "BLEU = 22.25 49.8/27.8/17.2/11.0 (BP = 0.983 ratio = 0.983 hyp_len = 2284 ref_len = 2324)\n",
    "chrF2++ = 47.89\n",
    "BLEU = 25.28 52.2/31.2/20.0/13.1 (BP = 0.989 ratio = 0.989 hyp_len = 2346 ref_len = 2371)\n",
    "chrF2++ = 51.87\n",
    "````\n",
    "\n",
    "Results with 4 beams:\n",
    "```\n",
    "V1\n",
    "BLEU = 24.14 52.5/30.4/18.9/12.1 (BP = 0.981 ratio = 0.981 hyp_len = 2281 ref_len = 2324)\n",
    "chrF2++ = 49.49\n",
    "BLEU = 23.41 52.1/31.0/18.9/11.3 (BP = 0.966 ratio = 0.967 hyp_len = 2292 ref_len = 2371)\n",
    "chrF2++ = 50.89\n",
    "V2\n",
    "BLEU = 25.18 52.4/31.3/20.4/13.3 (BP = 0.976 ratio = 0.976 hyp_len = 2269 ref_len = 2324)\n",
    "chrF2++ = 49.85\n",
    "BLEU = 23.22 51.6/29.4/18.3/11.6 (BP = 0.975 ratio = 0.975 hyp_len = 2312 ref_len = 2371)\n",
    "chrF2++ = 49.87\n",
    "V3\n",
    "BLEU = 23.06 51.1/29.1/18.1/11.5 (BP = 0.978 ratio = 0.978 hyp_len = 2273 ref_len = 2324)\n",
    "chrF2++ = 48.56\n",
    "BLEU = 26.12 53.4/32.5/21.0/13.6 (BP = 0.985 ratio = 0.985 hyp_len = 2336 ref_len = 2371)\n",
    "chrF2++ = 52.60\n",
    "```\n",
    "\n",
    "Which means:\n",
    "* For all directions and models, beam search improves the results\n",
    "* Longer training builds up quality for Tyvan, but decreases it for Russian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ywUwR9KfP0yt"
   },
   "source": [
    "```\n",
    "                                  | tyv->rus | rus->tyv\n",
    "Model v1 (no vocabulary update):  |\n",
    "    no beam search                |   23.21  |  22.03\n",
    "    num_beams = 4                 |   24.14  |  23.41\n",
    "Model v2 (with vocabulary update):|\n",
    "    no beam search                |   24.08  |  22.50\n",
    "    num_beams = 4                 |   25.18  |  23.22\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1mVG4Gy9KYK"
   },
   "source": [
    "## Publishing the model to HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r5PuJmlJ994_",
    "outputId": "ebfdfb8f-085a-43ad-ad86-dc1090ddddc3"
   },
   "outputs": [],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vtrBFlQp9hSb"
   },
   "outputs": [],
   "source": [
    "from transformers import NllbTokenizer, AutoModelForSeq2SeqLM, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RmGVeHIzFiuA"
   },
   "outputs": [],
   "source": [
    "def fix_tokenizer(tokenizer, new_lang='tyv_Cyrl'):\n",
    "    \"\"\" Add a new language token to the tokenizer vocabulary (this should be done each time after its initialization) \"\"\"\n",
    "    old_len = len(tokenizer) - int(new_lang in tokenizer.added_tokens_encoder)\n",
    "    tokenizer.lang_code_to_id[new_lang] = old_len-1\n",
    "    tokenizer.id_to_lang_code[old_len-1] = new_lang\n",
    "    # always move \"mask\" to the last position\n",
    "    tokenizer.fairseq_tokens_to_ids[\"<mask>\"] = len(tokenizer.sp_model) + len(tokenizer.lang_code_to_id) + tokenizer.fairseq_offset\n",
    "\n",
    "    tokenizer.fairseq_tokens_to_ids.update(tokenizer.lang_code_to_id)\n",
    "    tokenizer.fairseq_ids_to_tokens = {v: k for k, v in tokenizer.fairseq_tokens_to_ids.items()}\n",
    "    if new_lang not in tokenizer._additional_special_tokens:\n",
    "        tokenizer._additional_special_tokens.append(new_lang)\n",
    "    # clear the added token encoder; otherwise a new token may end up there by mistake\n",
    "    tokenizer.added_tokens_encoder = {}\n",
    "    tokenizer.added_tokens_decoder = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r6OR05X89a71",
    "outputId": "8b2a98f7-09c8-4461-9e8e-191c32d0d9d1"
   },
   "outputs": [],
   "source": [
    "model_load_name = '/gd/MyDrive/models/nllb-rus-tyv-v1'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_load_name)\n",
    "tokenizer = NllbTokenizer.from_pretrained(model_load_name)\n",
    "fix_tokenizer(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 150,
     "referenced_widgets": [
      "79d5f10ea79041c1bac7288656bca516",
      "ecb3db3792174c1a981da47b05bcf803",
      "ce3b6707fde344c2a716878a8cde3402",
      "cb9e3644d0fc4f3fa45efaa34cfc13c4",
      "57040992a43947ffa6ead81e184f7036",
      "5f244ff98c44422384025a25ad8fc779",
      "37bade299f834601b876cc1f200063fd",
      "aef8675be22c4b688533791c58d84dbb",
      "b9eb136229e949da91c0a03ad0f7f7af",
      "bc082bba84164c6e855253cc71801f3c",
      "6731bcd3e5104732b6c6b3917a5e2f90",
      "6cd6374c78f244548b79c11bcd089d16",
      "6902215d7f92404f99489865748286e3",
      "99389cae0f304a03819cd882f4ade40d",
      "ddc9b3ac87cb40b8b41a5d4bca98b2af",
      "310ac5268d70468a8ff4b53a938553af",
      "c687e45084444bc9a4bed6abe593f2ba",
      "9cf39baa725b4e0784c0113644efb60f",
      "b5bd0ea3f2ca4ee6b7a1f342274ff63a",
      "5a421731b36648168c7c417388219dad",
      "226a4c8a97ef4a359858d2cbbd13dcea",
      "bcaf9ac3a03d436b9322a14ce025611e"
     ]
    },
    "id": "zf0U6Vgf9qu-",
    "outputId": "3e80561f-a03a-4551-e6ad-3b81ee8253f6"
   },
   "outputs": [],
   "source": [
    "upload_repo = \"slone/nllb-rus-tyv-v1\"\n",
    "tokenizer.push_to_hub(upload_repo)\n",
    "model.push_to_hub(upload_repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eSKL2avG-WX_",
    "outputId": "effced5b-a9ac-4d41-ef2d-fd015680094b"
   },
   "outputs": [],
   "source": [
    "model_load_name = '/gd/MyDrive/models/nllb-rus-tyv-v2-extvoc'\n",
    "tokenizer = NllbTokenizer.from_pretrained(model_load_name)\n",
    "cfg = AutoConfig.from_pretrained(model_load_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_load_name + \"/pytorch_model_60k.bin\", config=cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 150,
     "referenced_widgets": [
      "362af541ede543768ce93e17f1d61bb2",
      "cc2ecd822b9544ca959fd9261616bc48",
      "3777e3a308fd4dda90ee586e5c10be1e",
      "96b23ce96ca24a48a394ffa896ba826b",
      "05a688fb436f4f198ae33b23a2a597d8",
      "d031b9c0721f49e3a6b0d834de8509b2",
      "3ae752dc3dee4c588a271c331c281e53",
      "b840fe71ba454d81ab26e64a6076e2b4",
      "9654591de7d44e20b22c88de1bb65bd1",
      "148b54598c4149b7975dd2a61c2b3b3e",
      "52a3480806dc48dd9223362e733ac3d7",
      "c1b1401e2ff848eaa56cd481811f5cb7",
      "ec6fdf7481424d66adac2b89f44945e4",
      "17d6237bbad7414c9d262d7834f568df",
      "d6769dad0c024ae786aca7a29f5f3c10",
      "2b8ede97bb094c2d8bcb24e00dd08ffe",
      "1722ec3afe0b4f439519c967038301cc",
      "954f57b73e554d09b6949aa58194008b",
      "dcc9e4ea553f4f5fa9aeeb38a945575a",
      "9bb9853a89704244bcbf0c1081402e0e",
      "1267c4ccdc1042f38f52bb0199ad0324",
      "d1f47279843e45c8b525ad14491b4e1d"
     ]
    },
    "id": "uldyz3aG-Y4H",
    "outputId": "717530ec-0240-4b83-b322-c310e4575906"
   },
   "outputs": [],
   "source": [
    "upload_repo = \"slone/nllb-rus-tyv-v2-extvoc\"\n",
    "tokenizer.push_to_hub(upload_repo)\n",
    "model.push_to_hub(upload_repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3zygd7HmFrLJ",
    "outputId": "e711c9d7-02b8-4886-911b-994915aac481"
   },
   "outputs": [],
   "source": [
    "print(tokenizer.convert_ids_to_tokens([256202, 256203, 256204])) # ['zul_Latn', 'tyv_Cyrl', '<mask>']\n",
    "print(tokenizer.convert_tokens_to_ids(['zul_Latn', 'tyv_Cyrl', '<mask>'])) # [256202, 256203, 256204]\n",
    "# this is consistent now, wow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 246,
     "referenced_widgets": [
      "50a0ef4daefd47b7b7b64009f358dc41",
      "24abf099591e4a899885de04b3fd8321",
      "6859e3c6c0984676b917f79b21b56dc8",
      "c8a6ec22038d400aa37d98c894dea92a",
      "f3e7dd3724cb4ad49a8bac488e245698",
      "dd4bc071514a47ef919e5590845b2a78",
      "73d6676d04a448ebbfc80102eb3a0e58",
      "1b0e75b61e574f678b9e3d36e0802ac3",
      "b17359c60aa24e029e112fddc7f28826",
      "90b1696cb54a4bcbba6c7402bba4b649",
      "7a6767005260467bbe539944b8b6e369",
      "1423cc55b5814e4c895bdee24201f0ec",
      "d545c961cdea43ad85d78b18691d72ab",
      "a67208795d664dcb9effab4a65e3b79e",
      "507cc289eb8a4fc19864d163d95ba5fc",
      "6e82afd16a3a4c9797fd8382c78e6046",
      "5aef36278ed942789d24bacd873c6f0b",
      "f3b8e4f9b9264146b2465c708351f65d",
      "3c460f721f424ac99c91dcac2fa4bbfa",
      "e48127d45b8e433e8d2ea1eae029c781",
      "cffce888c2604c8b9fe59c858603ddf9",
      "d2ecd924600a49908c9729c0903e35c4",
      "09961195b22648ea859f993290dd6dd3",
      "7b30ceacc6ac40a48ea32ff82b34428c",
      "08536b0168fe47c993102b0f46581842",
      "6f0bebdc37c64942923c577dada7d5b9",
      "41e13138581b4197a82e91cea90194a2",
      "ef47628e909d471781ce6f83ae88f2a9",
      "38d68291eac247cf85bfb1de50d8b120",
      "083bb600d64848589c02b9d772736a8c",
      "d062239d38e74048aadcc946bb974cb7",
      "3053da3f6f31407fb5ee0dc0402bfdb0",
      "ee2c90d133ed4fa69ffa5427e0e71f4e",
      "55593e579b654a4686a97175bc9372d2",
      "5313e9e56e3549068ae32738baf0b306",
      "d2b42ed99d254880b96d6bf5813cef56",
      "875fe414f74d4de989dcf249e1653aa6",
      "e620ef40b21640079d41d28b863eb013",
      "d1f84a81069042bdae75373de6536172",
      "eb5c4067edac458fbe71d8a324aa7408",
      "1617ee5bd47d403d874325bc0770c5b4",
      "9e9b0df4e1db4685b5fe64b0df76f522",
      "258e87e4744e4621acd3cbc64b5d2618",
      "8ab6cd9e36dc4da1b90a878b72504d0e",
      "43d03284a26148f89020a1001e226801",
      "19efa6d1430e4947888691bd06bc984f",
      "20a7f65a15be4081ba5b2848eb08a652",
      "5ebeb9328cb64491b80a8e6f956e74c9",
      "1ae9df2028994482a4a0c8ea90e94295",
      "14368928461046b8a478c6850a8c0309",
      "9b7cbfa1b1a0423a978fd657004ef995",
      "8284e1e6aea14607a8cb9d39d5fa8303",
      "491343173f17475b909aaa57a445b68b",
      "006504cc2cc34662a7686b5199b3fb3a",
      "7861e68bd3654ac580a4b70a5d9fa6f9",
      "9adc761447984736adc1d059197f389e",
      "f680c6102c2f4396ad9f0b6a5842a544",
      "c24bf754dca14ea6b2929391ecfb5e39",
      "5789aacf1599425e84eb25f6d0e1374a",
      "6f37a2f65b304e93bdb952fb76e4928a",
      "af0177c68cd9469c9f3229a12d5bbc00",
      "f742607592864f01befe3ac663f5e8b7",
      "4e32326f49d84389b198eb08ae529acd",
      "ed8ff220b68f46399040a0864a1434fc",
      "8e7b505016044ab18fffa81d80e12b24",
      "553cc9502a9a4ccc9c21efdcb08aa451"
     ]
    },
    "id": "0obaABVcQ3N_",
    "outputId": "39b0b63d-7986-454e-aa11-5b997ad337d1"
   },
   "outputs": [],
   "source": [
    "MODEL_URL = 'slone/nllb-rus-tyv-v1'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_URL)\n",
    "tokenizer = NllbTokenizer.from_pretrained(MODEL_URL, force_download=True)\n",
    "fix_tokenizer(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VIGg9OfFQ_vb"
   },
   "outputs": [],
   "source": [
    "def translate(\n",
    "    text,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    src_lang='rus_Cyrl',\n",
    "    tgt_lang='tyv_Cyrl',\n",
    "    max_length='auto',\n",
    "    num_beams=4,\n",
    "    no_repeat_ngram_size=4,\n",
    "    n_out=None,\n",
    "    **kwargs\n",
    "):\n",
    "    tokenizer.src_lang = src_lang\n",
    "    encoded = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    if max_length == 'auto':\n",
    "        max_length = int(32 + 2.0 * encoded.input_ids.shape[1])\n",
    "    model.eval()\n",
    "    generated_tokens = model.generate(\n",
    "        **encoded.to(model.device),\n",
    "        forced_bos_token_id=tokenizer.lang_code_to_id[tgt_lang],\n",
    "        max_length=max_length,\n",
    "        num_beams=num_beams,\n",
    "        no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "        num_return_sequences=n_out or 1,\n",
    "        **kwargs\n",
    "    )\n",
    "    out = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "    if isinstance(text, str) and n_out is None:\n",
    "        return out[0]\n",
    "    return out"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "machinetrans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
