{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure to (uv) pip install (-e) .\n",
    "\n",
    "from nllb_try.config import config\n",
    "from nllb_try.corpus import main_corpus\n",
    "\n",
    "# Tatoeba download niet meer nodig, doet TatoebaCorpus zelf\n",
    "# from nllb_try.downloadtatoeba import main_download\n",
    "# main_download(config[\"source_langs_tatoeba\"])\n",
    "\n",
    "# Step 2: Load and create parallel corpus\n",
    "corpus_objects = main_corpus(\n",
    "    config[\"source_langs_tatoeba\"],\n",
    "    config[\"source_langs_nllb\"],\n",
    "    variety_dir=\"data/RUG_data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming corpus_objects is your list\n",
    "\n",
    "for i, corpus in enumerate(corpus_objects):\n",
    "    print(f\"\\nCorpus {i}: {type(corpus).__name__}\")\n",
    "    print(f\"  Source NLLB label: {corpus.source_lang_nllb}\")\n",
    "    print(f\"  Target NLLB label: {corpus.target_lang_nllb}\")\n",
    "\n",
    "    # Sizes\n",
    "    df = corpus.df\n",
    "    df_train = corpus.df_train\n",
    "    df_validate = corpus.df_validate\n",
    "    print(f\"  Total rows:     {len(df)}\")\n",
    "    print(f\"  Train rows:     {len(df_train)}\")\n",
    "    print(f\"  Validate rows:  {len(df_validate)}\")\n",
    "\n",
    "    # Longest/shortest target\n",
    "    if len(df) > 0:\n",
    "        lengths = df[\"target_sentence\"].str.len()\n",
    "        longest_idx = lengths.idxmax()\n",
    "        shortest_idx = lengths.idxmin()\n",
    "        print(f\"  Longest target_sentence ({lengths[longest_idx]} chars):\")\n",
    "        print(f\"    {df.loc[longest_idx, 'target_sentence']}\")\n",
    "        # print(f\"  Shortest target_sentence ({lengths[shortest_idx]} chars):\")\n",
    "        # print(f\"    {df.loc[shortest_idx, 'target_sentence']}\")\n",
    "    else:\n",
    "        print(\"  No sentences in this corpus.\")\n",
    "\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nllb_try.train import main_train\n",
    "# Step 3: Train the model\n",
    "main_train(corpus_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nllb_try.tokenizer_and_model_setup import setup_model_and_tokenizer\n",
    "from nllb_try.config import config\n",
    "\n",
    "# Load the tokenizer and model (use params that your training uses):\n",
    "model, tokenizer = setup_model_and_tokenizer(\n",
    "    config[\"modelname\"],\n",
    "    config[\"modelpath\"],\n",
    "    config[\"new_lang_nllb\"],\n",
    "    config[\"similar_lang_nllb\"],\n",
    "    device=config['device']\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def estimate_max_length_and_plot_outliers(tokenizer, sentences, percentile=98, sample_size=5000, n_outliers=5, title='Token Length Distribution'):\n",
    "    \"\"\"\n",
    "    Estimate max_length, plot distribution, and print the most extreme outliers.\n",
    "    \"\"\"\n",
    "    n = len(sentences)\n",
    "    size = min(sample_size, n)\n",
    "    sample_idxs = np.random.choice(n, size=size, replace=False)\n",
    "    sample = [sentences[i] for i in sample_idxs]\n",
    "    lengths = [len(tokenizer.tokenize(s)) for s in sample]\n",
    "    suggested = int(np.percentile(lengths, percentile))\n",
    "\n",
    "    # Stats\n",
    "    print(\n",
    "        f\"Estimated max_length at {percentile}th percentile: {suggested}\\n\"\n",
    "        f\"Sample size={size}, max={max(lengths)}, mean={np.mean(lengths):.1f}, median={np.median(lengths):.1f}\"\n",
    "    )\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.hist(lengths, bins=range(0, max(lengths)+1, 1), alpha=0.7, color='dodgerblue', edgecolor=\"black\")\n",
    "    plt.axvline(suggested, color=\"crimson\", linestyle=\"dashed\", lw=2,\n",
    "                label=f\"{percentile}th percentile ({suggested})\")\n",
    "    plt.xlabel(\"Tokenized Sentence Length\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Report outliers (show longest n_outliers)\n",
    "    print(f\"\\nTop {n_outliers} outliers (longest in sample):\")\n",
    "    lengths_np = np.array(lengths)\n",
    "    outlier_idxs = lengths_np.argsort()[-n_outliers:][::-1]\n",
    "    for i in outlier_idxs:\n",
    "        print(f\"\\n[{lengths[i]} tokens]\\n{sample[i][:500]}\")  # Show up to 500 chars\n",
    "\n",
    "    return suggested\n",
    "\n",
    "source_sentences = []\n",
    "target_sentences = []\n",
    "for corpus in corpus_objects:\n",
    "    source_sentences.extend(corpus.df_train['source_sentence'].tolist())\n",
    "    target_sentences.extend(corpus.df_train['target_sentence'].tolist())\n",
    "\n",
    "percentile = 98\n",
    "max_length_src = estimate_max_length_and_plot_outliers(\n",
    "    tokenizer, source_sentences, percentile, title='Source Sentence Token Lengths'\n",
    ")\n",
    "max_length_tgt = estimate_max_length_and_plot_outliers(\n",
    "    tokenizer, target_sentences, percentile, title='Target Sentence Token Lengths'\n",
    ")\n",
    "optimal_max_length = max(max_length_src, max_length_tgt)\n",
    "print(f\"\\nSuggested max_length for training: {optimal_max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nllb_try.tryout import main_tryout\n",
    "from nllb_try.config import config\n",
    "MODEL_SAVE_PATH = config[\"MODEL_SAVE_PATH\"]\n",
    "print(MODEL_SAVE_PATH)\n",
    "\n",
    "input_sentences_nld = [\n",
    "    \"Ik hou van kaas. Midden van de nacht, laat het razen.\",\n",
    "    \"ik hou van chocolade.\",\n",
    "    \"De zon schijnt helder. Ik heb mijn zonnebril op.\",\n",
    "    \"de zon schijnt fel vandaag.\",\n",
    "    \"Zij leest een boek.\",\n",
    "    \"het boek ligt op de tafel.\",\n",
    "    \"Wij drinken water, zei de kater.\",\n",
    "    \"De kat slaapt zachtjes. Groetjes van oma.\",\n",
    "    \"we gaan morgen naar oma.\",\n",
    "    \"de straatkat is een kat die op straat leeft.\",\n",
    "    \"de hond blaft naar de kat.\",\n",
    "    \"ik heb een hekel aan regen.\",\n",
    "    \"Het regent buiten. Neem een paraplu mee!\",\n",
    "    \"ik ben benieuwd of dit werkt.\",\n",
    "    \"Jij bent het zusje van mijn buurman!\",\n",
    "    \"Onverwachts kwamen de ouders van Bert op bezoek.\",\n",
    "    \"Hebben jullie dat gezegd?\",\n",
    "    \"de molenaar ziet jullie.\",\n",
    "    \"ik weet niet wat je bedoelt...\",\n",
    "    \"wiens linkerarm is het sterkste? die van mij.\",\n",
    "    \"Hoe graag wil je het hebben?\",\n",
    "    \"het is vandaag mooi weer.\",\n",
    "    \"die film was echt heel spannend.\",\n",
    "    \"de vogel zingt een mooi liedje.\",\n",
    "    \"het kind speelt in de tuin.\",\n",
    "    \"wat had jij daarop kunnen zeggen?\"\n",
    "]\n",
    "\n",
    "print(\"--- initial translation (nl to gos) ---\")\n",
    "\n",
    "translations_gos = main_tryout(\n",
    "    MODEL_SAVE_PATH,\n",
    "    config[\"new_lang_nllb\"],\n",
    "    input_sentences_nld,\n",
    "    src_lang='nld_Latn',\n",
    "    tgt_lang='gos_Latn'\n",
    ")\n",
    "\n",
    "print(\"\\n--- backtranslation (gos to nl) ---\")\n",
    "# Step 3: Use the output of the first translation for back-translation (Gos to Dutch)\n",
    "back_translations_nld = main_tryout(\n",
    "    MODEL_SAVE_PATH,\n",
    "    config[\"new_lang_nllb\"],\n",
    "    translations_gos,\n",
    "    src_lang='gos_Latn',\n",
    "    tgt_lang='nld_Latn'\n",
    ")\n",
    "\n",
    "print(\"\\n--- paired results ---\")\n",
    "for i, original_nld in enumerate(input_sentences_nld):\n",
    "    print(f\"Original Dutch:     {original_nld}\")\n",
    "    print(f\"Translated Gos:     {translations_gos[i]}\")\n",
    "    print(f\"Back-Translated Dutch: {back_translations_nld[i]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the different training stages of the model\n",
    "# MODEL_SAVE_PATH = config[\"MODEL_SAVE_PATH\"]\n",
    "from nllb_try.evaluate import main_evaluate\n",
    "main_evaluate(corpus_objects, MODEL_SAVE_PATH, \"gos_Latn\", config[\"timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nllb_try.config import config\n",
    "# MODEL_SAVE_PATH = \"/home/tom.brand/Offline/nllb-tryout/notebooks/checkpoints/nllb-200-distilled-600M-nld-gos-20251207-220722/\"\n",
    "\n",
    "import os\n",
    "\n",
    "from nllb_try.tokenizer_and_model_setup import setup_model_and_tokenizer\n",
    "model_path = os.path.join(MODEL_SAVE_PATH, \"epoch12\") # select the epoch that scores best\n",
    "print(f\"Loading model from {model_path}...\")\n",
    "model, tokenizer = setup_model_and_tokenizer(model_path, new_lang=config[\"new_lang_nllb\"], device='cuda')\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nllb_try.tryout import translate\n",
    "print(translate(\n",
    "    'De zon schijnt helder. Ik heb mijn zonnebril op.',\n",
    "    src_lang='nld_Latn',\n",
    "    tgt_lang='gos_Latn',\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_hub.login(token=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_repo = \"Tom9358/nllb-tatoeba-gos-nld-v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.push_to_hub(upload_repo)\n",
    "model.push_to_hub(upload_repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def print_directory_structure(base_dir: str):\n",
    "    \"\"\"\n",
    "    Print de structuur van mappen binnen de basisdirectory, tot twee niveaus diep.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        first_level = os.listdir(base_dir)\n",
    "        print(f\"Eerste niveau in '{base_dir}' bevat:\")\n",
    "        print(first_level)\n",
    "\n",
    "        for first_level_dir in first_level:\n",
    "            first_level_path = os.path.join(base_dir, first_level_dir)\n",
    "            if os.path.isdir(first_level_path):\n",
    "                second_level = os.listdir(first_level_path)\n",
    "                print(f\"\\nTweede niveau in '{first_level_dir}' bevat:\")\n",
    "                print(second_level)\n",
    "    except Exception as e:\n",
    "        print(f'Er is een fout opgetreden: {e}')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_directory = 'models'\n",
    "    print_directory_structure(base_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
